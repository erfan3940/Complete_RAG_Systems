{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0720978",
   "metadata": {},
   "source": [
    "### with this coding journey to the sound by ML we can answer this question \"what is this sound\" by its waveform or mel spectrogram picture with the help of CNN and ResNet50 by some tricks for model enhacement\n",
    "\n",
    "**Dataset** : ESC-50 (environmental sound classification) with 50 classes(e.g., barking, snoring, ...)\n",
    "\n",
    "**Libraryies** : torchaudio with two other pieces of software: SoX and LibROSA\n",
    "\n",
    "**models** : \n",
    "1. CNN (convolutional Neural Network) using ID variants \n",
    "2. pretrained ResNet50 for Mel Spectrogram\n",
    "\n",
    "**specials** : \n",
    "1. find_lr() function to find a decent learning rate better than GridSearch \n",
    "2. frequency masking (in the future)\n",
    "3. time masking (in the future)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3bf1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38fbd20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2481897942.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    curl https://github.com/karoldvl/Esc-50/archive/master.zip\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "curl https://github.com/karoldvl/Esc-50/archive/master.zip\n",
    "import Ipython.display as display\n",
    "display.Audio('Esc-50/audio/1-100032-A-0.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "list = [f.split(\"-\")[-1].replace(\".wav\",\"\") for f in glob.glob(\"ESC-50/audio/*.wav\")]\n",
    "Counter(list) # convert to dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43f770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sox torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd649fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building an ESC-50 dataset\n",
    "\n",
    "class ESC50(Dataset):\n",
    "    def __init__(self, path):\n",
    "        files = path(path).glob(\"*.wav\")\n",
    "        # iterate through the listing and create tuple (filename,label)\n",
    "        self.items = [f,f.split(\"-\")[-1].replace(\".wav\",\"\") for f in files]\n",
    "        self.length = len(self.items)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        filename, label = self.items[index]\n",
    "        audio_tensor, sample_rate = torchaudio.load(filename)\n",
    "        return audio_tensor, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "test_esc50 = ESC50(\"mypath\")\n",
    "tensor, label = list(test[0])\n",
    "\n",
    "tensor\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8703f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split data into 60/20/20\n",
    "first number present 20 percent of dataset so :\n",
    "1 2 3 for train\n",
    "4 for valid\n",
    "5 for test\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "bs = 64\n",
    "PATH_TO_ESC-50 = Path.cwd() / 'esc50'\n",
    "path = 'test.md'\n",
    "test\n",
    "\n",
    "train = Path(PATH_TO_ESC-50/ \"train\")\n",
    "valid = Path(PATH_TO_ESC-50/ \"valid\")\n",
    "test = Path(PATH_TO_ESC-50/ \"test\")\n",
    "\n",
    "train_loader = torch.utils.DataLoader(train, batch_size = bs, shuffle = true)\n",
    "valid_loader = torch.utils.DataLoader(valid, batch_size = bs, shuffle = true)\n",
    "test_loader = torch.utils.DataLoader(test, batch_size = bs, shuffle = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79df3105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model for dataset\n",
    "\n",
    "class AudioNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgPool = nn.AvgPool(30)\n",
    "        self.fc1 = nn.Linear(512,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgPool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cd43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audionet = AudioNet()\n",
    "audionet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f00bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_lr() function\n",
    "import math\n",
    "def find_lr(model, loss_fn, optimizer, init_value=1e-8, final_value=10.0):\n",
    "number_in_epoch = len(train_loader) - 1\n",
    "update_step = (final_value/init_value) ** (1/number_in_epoch)\n",
    "lr = init_value\n",
    "optimizer.param_groups[0][\"lr\"] = lr\n",
    "best_loss = 0.0\n",
    "batch_num = 0\n",
    "losses = []\n",
    "log_lrs = []\n",
    "for data in train_loader:\n",
    "    batch_num += 1\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs, labels\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    # crash out if loss explodes\n",
    "    if batch_num > 1 and loss > 4 * best_loss:\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "    # record the best loss\n",
    "    if batch_num == 1 or loss < best_loss:\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "\n",
    "    # store the value\n",
    "    losses.append(loss)\n",
    "    log_lrs.append(math.log10(lr))\n",
    "    \n",
    "    # do backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update the lr for the next step\n",
    "    lr *= update_step\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "return log_lrs[10:-5], losses[10:-5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audionet.save(\"audionet.pth\")\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(audionet.parameters(), lr=0.001)\n",
    "logs, losses = find_lr(audionet, nn.CrossEntropyLoss(), optimizer)\n",
    "plt.plot(logs,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4754d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train model for 20 epochs\n",
    "# expect to be 13% better than 3% by the chance\n",
    "train(audionet, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, valid_data_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652cabcb",
   "metadata": {},
   "source": [
    "lets investigate a way of looking at our audio data that may yield better results, maybe by transfer learning with ResNet50 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample, sr = libsora.load(\"ESC-50/train/1-100032-A-0.wav\", sr=None)\n",
    "spectrogram = libsora.feature.melspectrogram(sample, sr=sr)\n",
    "spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2868638",
   "metadata": {},
   "outputs": [],
   "source": [
    "libsora.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb1c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covert to logarithmic for better scale\n",
    "log_spectrogram = libsora.power_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a4ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a new dataset\n",
    "\n",
    "class ESC50Spectrogram(Dataset):\n",
    "    def __init__(self, path):\n",
    "        files = path(path).glob(\"*.wav\")\n",
    "        # iterate through the listing and create tuple (filename,label)\n",
    "        self.items = [f,f.split(\"-\")[-1].replace(\".wav\",\"\") for f in files]\n",
    "        self.length = len(self.items)\n",
    "        self.transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        filename, label = self.items[index]\n",
    "        audio_tensor, sample_rate = libsora.load(filename, sr=None)\n",
    "        spectrogram = libsora.feature.melspectrogram(audio_tensor, sr=sample_rate)\n",
    "        log_spectrogram = libsora.power_to_db(spectrogram, ref=np.max)\n",
    "        libsora.display.specshow(spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "\n",
    "        plt.gcf().canvas.draw()\n",
    "        audio_data = audio_data.reshape(fig.canvas.get_width_heigth()[::-1] + (3,))\n",
    "        return (self.transforms(audio_data), label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b6eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dataset = ESC50(\"ESC-50/train/\")\n",
    "start_time = time.process_time()\n",
    "old_dataset.__getitem__(33)\n",
    "end_time = tme.process_time()\n",
    "old_time = end_time - start_time\n",
    "\n",
    "new_dataset = ESC50Spectrogram(\"ESC-50/train/\")\n",
    "start_time = time.process_time()\n",
    "new_dataset.__getitem__(33)\n",
    "end_time = tme.process_time()\n",
    "new_time = end_time - start_time\n",
    "\n",
    "print(f\"new time: {new_time}\\nold time: {old_time}\") # its one hundred times slower so we should use lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a150950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "class ESC50Spectrogram(Dataset):\n",
    "    def __init__(self, path):\n",
    "        files = path(path).glob(\"*.wav\")\n",
    "        # iterate through the listing and create tuple (filename,label)\n",
    "        self.items = [f,f.split(\"-\")[-1].replace(\".wav\",\"\") for f in files]\n",
    "        self.length = len(self.items)\n",
    "        self.transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "    @functools.lru_cache(maxsize=<size of dataset>)\n",
    "    def __getitem__(self,index):\n",
    "        filename, label = self.items[index]\n",
    "        audio_tensor, sample_rate = libsora.load(filename, sr=None)\n",
    "        spectrogram = libsora.feature.melspectrogram(audio_tensor, sr=sample_rate)\n",
    "        log_spectrogram = libsora.power_to_db(spectrogram, ref=np.max)\n",
    "        libsora.display.specshow(log_spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "\n",
    "        plt.gcf().canvas.draw()\n",
    "        audio_data = audio_data.reshape(fig.canvas.get_width_heigth()[::-1] + (3,))\n",
    "        return (self.transforms(audio_data), label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551036c",
   "metadata": {},
   "source": [
    "my prefer approach is to precompute all possible plots and then create a new custom datset that load images from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d55c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_spectrogram(path, dpi=50):\n",
    "    files = Path(path).glob('*.wave')\n",
    "    for filename in files:\n",
    "        audio_tensor, sample_rate = libsora.load(filename, sr=None)\n",
    "        log_spectrogram = libsora.power_to_db(spectrogram, ref=np.max)\n",
    "        libsora.display.specshow(log_spectrogram, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "        plt.gcf().savefig(\"{}{}_{}.png\".format(filename.parant, dpi, filename.name),dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b8771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we cant use ImageDataLoader because of filename but no matter we make our own\n",
    "from PIL import ImageDataLoader\n",
    "\n",
    "class precompute(Dataset):\n",
    "    def __init__(self,path,dpi=50,transforms=None):\n",
    "        files = Path(path).glob(\"{}*.wav.png\".format(dpi))\n",
    "        self.items = [(f, int(f.name.split(\"-\")[-1].replace(\".wave.png\",\"\"))) for f in files]\n",
    "        self.length = len(self.items)\n",
    "        if transforms=None:\n",
    "            self.transforms = torchvision.transforms.Compose[(torchvision.transforms.ToTensor())]\n",
    "        else:\n",
    "            self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        filename, label = self.items[index]\n",
    "        img = Image.open(filename)\n",
    "        return (self.transforms(img), label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce115569",
   "metadata": {},
   "outputs": [],
   "source": [
    "precompute_dataset = precompute(\"ESC-50/train/\")\n",
    "start_time = time.process_time()\n",
    "precompute_dataset.__getitem__(33)\n",
    "end_time = tme.process_time()\n",
    "new_time = end_time - start_time\n",
    "\n",
    "print(f\"new time: {new_time}\") # this one is as same as audio-base one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40009da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model\n",
    "from torchvision import models\n",
    "resnet = models.ResNet50(pretrained=True)\n",
    "\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = false\n",
    "\n",
    "resnet.fc = nn.Sequential(nn.Linear(resnet.fc.in_features,500),nn.ReLU(),nn.Dropout(), nn.Linear(500,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b6182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also need to normalize the incoming images with the standard deviation and mean\n",
    "\n",
    "esc50-train = PreparedESC50(PATH, transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "esc50-valid = PreparedESC50(PATH, transforms=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "esc_train_loader = (esc50-train, bs, shuffle=True)\n",
    "esc_valid_loader = (esc50-valid, bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding learning rate\n",
    "resnet.save(\"resnet.pth\")\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "logs, losses = find_lr(resnet, nn.CrossEntropyLoss(), optimizer)\n",
    "plt.plot(logs,losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cfbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=[1e-2, 1e-4, 1e-8])\n",
    "train(resnet, optimizer, nn.CrossEntropyLoss(), esc_train_loader, esc_valid_loader, epochs=5, device=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
