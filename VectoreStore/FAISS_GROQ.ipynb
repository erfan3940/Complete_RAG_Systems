{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eef1d32",
   "metadata": {},
   "source": [
    "### Building a RAG System with LangChain and FAISS \n",
    "\n",
    "pros:\n",
    "1. Extremely fast similarity search\n",
    "2. Memory efficient\n",
    "3. Supports GPU acceleration\n",
    "4. Can handle millions of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b129eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "from typing import List,Dict,Any\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# langChain core imports\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# langChain specific imports\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer                #hugging_face Embedding\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader, PyMuPDFLoader\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056a551",
   "metadata": {},
   "source": [
    "import from pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "28ec92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "class PDFProcessor:\n",
    "    \"\"\"complete PDF processing for embedding\"\"\"\n",
    "\n",
    "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\",\"\\n\",\" \",\"\"],\n",
    "        )\n",
    "\n",
    "    def preprocess(self, pdf_path: str) -> List[Document]:\n",
    "        \"\"\"Load and clean and chunk a single PDF\"\"\"\n",
    "        pdf_path = str(pdf_path)\n",
    "        file_name = Path(pdf_path).name\n",
    "\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(pdf_path)\n",
    "            pages = loader.load()\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load {pdf_path}: {e}\")\n",
    "\n",
    "        final_chunks = []\n",
    "\n",
    "        for page_idx, page in enumerate(pages):\n",
    "            cleaned_text = self._clean_text(page.page_content)\n",
    "\n",
    "            # skip empty and garbage pages\n",
    "            if len(cleaned_text) < 50:\n",
    "                continue\n",
    "            \n",
    "            if self._is_garbled_text(cleaned_text):\n",
    "                print(f\"Skipping garbled page {page_idx+1} in {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            chunks = self.text_splitter.create_documents(\n",
    "                texts=[cleaned_text],\n",
    "                metadatas=[{\n",
    "                    \"file_name\": file_name,\n",
    "                    \"source\": pdf_path,\n",
    "                    \"page\": page_idx + 1,\n",
    "                    \"total_pages\": len(pages),\n",
    "                    \"chunk_method\": \"custom PDFProcessor\",\n",
    "                    \"word_count\": len(cleaned_text.split()),\n",
    "                    \"char_count\": len(cleaned_text),\n",
    "                }],\n",
    "            )\n",
    "\n",
    "            final_chunks.extend(chunks)\n",
    "\n",
    "        print(f\"{file_name}: {len(final_chunks)} chunks created\")\n",
    "        return final_chunks\n",
    "\n",
    "    def _is_garbled_text(self, text: str) -> bool:\n",
    "        if not text:\n",
    "            return True\n",
    "\n",
    "        # Count control characters\n",
    "        control_chars = sum(1 for c in text if ord(c) < 32 and c not in \"\\n\\t\")\n",
    "\n",
    "        # Persian letters range\n",
    "        persian_letters = len(re.findall(r\"[آ-ی]\", text))\n",
    "\n",
    "        # Heuristics\n",
    "        if control_chars > 20:\n",
    "            return True\n",
    "        if persian_letters < 10:\n",
    "            return True\n",
    "        if persian_letters / max(len(text), 1) < 0.02:\n",
    "            return True\n",
    "\n",
    "        return False      \n",
    "    \n",
    "    def _clean_text (self,text) -> str:\n",
    "            # 1. Unicode normalization (SAFE)\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # 2. Remove control characters\n",
    "        text = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f]\", \"\", text)\n",
    "\n",
    "        # 3. Normalize whitespace (but keep word boundaries!)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77079010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to extract pdf path from dirloader (dont use loader_cls=TextLoader,)\n",
    "def get_all_pdf_paths(directory: str) -> list[str]:\n",
    "    loader = DirectoryLoader( # didnt work bcz try to use default UnstructuredPDFLoader\n",
    "                              # which depend on other libraries to be installed (slow and bad results in persian pdfs)\n",
    "        directory,\n",
    "        glob=\"**/*.pdf\",\n",
    "        show_progress=True\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    # extract unique file paths\n",
    "    paths = list({doc.metadata[\"source\"] for doc in docs})\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "38b37bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new rubust and easy way to get paths\n",
    "def new_get_all_pdf_paths(directory: str) -> list[str]:\n",
    "    \n",
    "    return [str(p) for p in Path(directory).rglob(\"*.pdf\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "351f3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_pdfs (data_dir: str) -> list[Document]:\n",
    "    processor = PDFProcessor(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    pdf_paths = new_get_all_pdf_paths(data_dir)\n",
    "    print(f\"Found {len(pdf_paths)} PDFs\")\n",
    "\n",
    "    all_chunks = []\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        chunks = processor.preprocess(pdf_path)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    print(f\"total chunks created: {len(all_chunks)} from {len(pdf_paths)} pdfs\")\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8d3bb84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 PDFs\n",
      "آموزش بازی بیلیارد.pdf: 15 chunks created\n",
      "MuPDF error: format error: No default Layer config\n",
      "\n",
      "آموزش_تعمیر_اتومبیلهای_سواری.pdf: 55 chunks created\n",
      "ابلاغیه.pdf: 0 chunks created\n",
      "Skipping garbled page 1 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 2 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 3 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 4 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 5 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 6 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 7 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 8 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 9 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 10 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 11 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 12 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 13 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 14 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 15 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 16 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 17 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "Skipping garbled page 18 in مدار_گاز_سوز_اتوبوس_های_شهری.pdf\n",
      "مدار_گاز_سوز_اتوبوس_های_شهری.pdf: 0 chunks created\n",
      "Skipping garbled page 1 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 2 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 3 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 4 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 5 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 6 in موتور_احتراق_داخلی.pdf\n",
      "Skipping garbled page 7 in موتور_احتراق_داخلی.pdf\n",
      "موتور_احتراق_داخلی.pdf: 0 chunks created\n",
      "Skipping garbled page 1 in کیسه_هوا.pdf\n",
      "Skipping garbled page 2 in کیسه_هوا.pdf\n",
      "کیسه_هوا.pdf: 0 chunks created\n",
      "total chunks created: 70 from 6 pdfs\n"
     ]
    }
   ],
   "source": [
    "documents = load_and_process_pdfs(\"G:/extracted/ML/RAG/data_management/pdf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57d5ed03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "آن است، تـاب برداشـتن اسـت و بـراي مشخص كردن مي بايست از يك خط كش فلزي استفاده كنيم . - عيب ديگر : پيچيدگي در سر سيلندر است كه تا حدودي قابل تعميرا ست در صورتيكه بيشـتر از اندازه مجاز باشد قابل تعمير نيست مثلاً از حدود 20 /0 ميليمتر بيشتر باشد قابـل تعميـر نخواهـد .بود عيب ديگر : مربوط به فنر سوپاپ در سرسيلندر است، مي بايست فنرهاي سوپاپ هوا و دود را در كنار هم قرار دهيم و مي بايست همگي يك انازه باشند . در صو رت كوتـاه بـودن آن فنـري كـه كوتاه بود قابل تعمير نيست و باعث سوختن سوپاپ مي شود . واشر\n"
     ]
    }
   ],
   "source": [
    "print(documents[55].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929e0e9b",
   "metadata": {},
   "source": [
    "### Embedding using HooshvareLab/distilbert-fa-zwnj-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a3711f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name HooshvareLab/distilbert-fa-zwnj-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values: [0.02403348870575428, -0.01216572429984808, 0.023973509669303894, -0.006327626761049032, 0.011993647553026676]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(\n",
    "    model_name=\"HooshvareLab/distilbert-fa-zwnj-base\",\n",
    "    model_kwargs={\"device\": \"cpu\"},   # or cuda for gpu\n",
    "    encode_kwargs={\n",
    "        \"normalize_embeddings\": True,  # so important for cosine similarity\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    ")\n",
    "embeddings = embedding_function.embed_documents([documents[20].page_content])[0]\n",
    "print(\"First 10 values:\", embeddings[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aaba13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilingual hugging face\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embedding_function2 = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "78b0d864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created with 70 vectors\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_function2\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33442ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.save_local(\"faiss_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e5e54f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x205369e9750>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.load_local(\n",
    "    \"faiss_db\",\n",
    "    embeddings=embedding_function,\n",
    "    allow_dangerous_deserialization=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d9604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='e08f8969-349d-4d3e-b42e-a0566f99f5ac', metadata={'file_name': 'آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'source': 'G:\\\\extracted\\\\ML\\\\RAG\\\\data_management\\\\pdf_files\\\\آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'page': 11, 'total_pages': 22, 'chunk_method': 'custom PDFProcessor', 'word_count': 368, 'char_count': 1672}, page_content='بـه ايـن حركـت در بالاي خود يك حجمي را ايجاد مي كند كه اين حجم ايجاد شـده و خـلاء حاصـله و همينطـور زمانبندي كه براي سوپاپها در نظر گرفته شده، سوپاپ هوا باز مي شود و بـا بـاز شـدن سـوپاپ هوا، خلا حاصله در نتيجه پائين رفتن پيستون ، توسط مخلوط سوخت و هوايي كه كاربراتور انجم داده، به فضاي بالاي پيستون راه پيدا مي كند و مرحله مكش انجام مي گيرد . زمان دوم : با ادامه حركت ميل لنگ و گردش به سمت بـالا، پيسـتون هـم بـه سـمت بـالا حركت داده مي شود و هم زمان با اين عمل سوپاپ ورودي كه در مرحله مكش بـاز'),\n",
       " Document(id='336c4479-602a-48b8-9069-4ae261b4385b', metadata={'file_name': 'آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'source': 'G:\\\\extracted\\\\ML\\\\RAG\\\\data_management\\\\pdf_files\\\\آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'page': 11, 'total_pages': 22, 'chunk_method': 'custom PDFProcessor', 'word_count': 368, 'char_count': 1672}, page_content='پيسـتون هـم بـه سـمت بـالا حركت داده مي شود و هم زمان با اين عمل سوپاپ ورودي كه در مرحله مكش بـاز شـده بـود، حالا بستر مي شود و با بالارفتن پيستون، سوخت و هواي مكيده شـده در مرحلـه مكـش مـورد تراكم قرار مي گيرد و در فضاي اطاق احتراق متراكم مي شود . زمان سوم : با رسيدن پيستون به نقطه مرگ بالا، جرقه زني شمع انجام مي گيرد و باعث مي شود كه مخلوط سوخت و هواي ف شرده شده محترق بشود و به سـطح پيسـتون فشـار بيـاورد و پيستون را به سمت پائين حركت بدهد، حركت به پيستون به سمت پائين در واقع بـا ايجـاد كـار'),\n",
       " Document(id='ee1fe722-aa18-4bc1-a007-865c06285dea', metadata={'file_name': 'آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'source': 'G:\\\\extracted\\\\ML\\\\RAG\\\\data_management\\\\pdf_files\\\\آموزش_تعمیر_اتومبیلهای_سواری.pdf', 'page': 22, 'total_pages': 22, 'chunk_method': 'custom PDFProcessor', 'word_count': 194, 'char_count': 887}, page_content='. در هنگام بستن قطعات موتور مي بايست قطعات عاري از هر گونه آلودگي باشد مثل چربي ها . سپس ميل لنگ را روي ياتاقانها ق رار مي دهيم به صورت آرام سپس كفه ها را روي آن قرار مي ، سپس بغل ياتاقانها را \\ue820دهيم و با دست آنرا فشار مي دهيم تا جا بيافتد و پيچها را سفت مي كنيم قرار مي دهيم، سپس پيچها را به صورت حلزوني مي بنديم . نكته : بايد توجه داشت يك اتومبيل هر پيچ با گشتاور معيني محكم مي شود، براي ايـن كـار از آچار مخصوص بنام توكمتر استفاده مي كنيم . شبكه آموزش تنظيم : mohammad6347@gmail.com 22')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = vectorstore.similarity_search(\n",
    "    \"فنرهاي سوپاپ هوا و دود را در كنار هم قرار دهيم\",\n",
    "    k=3,\n",
    "    )\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b66957b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Simple RAG Chain with LCEL\n",
    "simple_prompt = ChatPromptTemplate.from_template(\"\"\"بر اساس متن اطلاعات موجود به سوال جواب بده اگر جواب را پیدا نکردی بگو نمیدانم\n",
    "اطلاعات: {context}\n",
    "\n",
    "سوال: {question}\n",
    "\n",
    "جواب:\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cf026226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020534F732F0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":3}\n",
    ")\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "aeed527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using modern lcel\n",
    "# Format documents for the prompt\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    \"\"\"Format documents for insertion into prompt\"\"\"\n",
    "    formatted = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        formatted.append(f\"Document {i+1} (Source: {source}):\\n{doc.page_content}\")\n",
    "    return \"\\n\\n\".join(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "32787fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.chat_models.base import init_chat_model\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API\"]=os.getenv(\"GROQ_API\")\n",
    "groq_llm = ChatGroq(model=\"llama-3.1-8b-instant\",api_key=os.getenv(\"GROQ_API\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "572bb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rag_chain=(\n",
    "    {\"context\":retriever | format_docs,\"question\":RunnablePassthrough() }\n",
    "    | simple_prompt\n",
    "    | groq_llm\n",
    "    |StrOutputParser()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ed6c6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversational RAg Chain\n",
    "conversational_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"بر اساس اطلاعات موجود در متن داده شده به سوال جواب بده\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"اطلاعات: {context}\\n\\n سوال: {input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "566344c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversational_rag():\n",
    "    return (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "        )\n",
    "        | conversational_prompt\n",
    "        | groq_llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "conversational_rag = create_conversational_rag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "edb34f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt\n",
    "    | groq_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "77893761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_chains(question: str):\n",
    "    \"\"\"Test all RAG chain \"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    #  Simple RAG\n",
    "    print(\"\\n1. Simple RAG Chain:\")\n",
    "    answer = simple_rag_chain.invoke(question)\n",
    "    print(f\"Answer: {answer}\")\n",
    "\n",
    "    print(\"\\n2. Streaming RAG:\")\n",
    "    print(\"Answer: \", end=\"\", flush=True)\n",
    "    for chunk in streaming_rag_chain.stream(question):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "884eb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: تعداد توپ های بیلیارد و توپ های خط دار\n",
      "================================================================================\n",
      "\n",
      "1. Simple RAG Chain:\n",
      "Answer: بر اساس اطلاعات موجود در سند 1، تعداد توپ های بیلیارد از 1 تا 51 است. همچنین، بر اساس سند 2، توپ های خط دار از شماره 9 تا 51 تشکیل می شوند.\n",
      "\n",
      "2. Streaming RAG:\n",
      "Answer: بر اساس اطلاعات موجود در Documents 1 و 2، تعداد توپ های بیلیارد از شماره 1 تا 151 است. از این تعداد، توپ های ساده از شماره 1 تا 7 و توپ های خط دار از شماره 9 تا 51 را تشکیل می دهند. همچنین، توپ 8 خاص است و جزو هیچ گروهی محسوب نمی شود.\n",
      "\n",
      "بنابراین، تعداد توپ های بیلیارد 151 نفر است و تعداد توپ های خط دار 43 نفر است (از شماره 9 تا 51).\n"
     ]
    }
   ],
   "source": [
    "test_rag_chains(\"تعداد توپ های بیلیارد و توپ های خط دار\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e96e9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'خوب به رنگ ها نگاه يد آن , نخستين چيز آه بايد درباره ي بيلي يارد بدانيد اين است آه اين ب ازي برخلاف باري ها ي توپي ديگر داراي توپ هاي زيادي است . برخي از اين توپ ها ساده يعني داراي رنگي يك پارچه هستند و تعدادي از آنها نيز خط دارند يعني توپي آه نوك و ته آن ها سفيد و يك خط رنگي آه گرداگرد و ميانه ي آن قرار گرفته . هر يك از توپ ها دار اي يك شماره هستند . از شماره ي تا ١ ٥١ , آه از شماره ي ١ تا ٧ تو پ هاي ساده واز شماره ي ٩ تا ٥١ توپ هاي خط دارد را تشكيل مي دهند . توپ شماره ي ٨ ٨ballآدام است ؟ .'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a4ba1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
